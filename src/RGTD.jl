

struct Prediction end
struct Derivative end
struct DoubleDerivative end

sigmoid(x, type::Type{Prediction}) = sigmoid(x)
sigmoid(x, type::Type{Derivative}) = sigmoidâ€²(x)
sigmoid(x, type::Type{DoubleDerivative}) = sigmoidâ€²â€²(x)

mutable struct GradientGVFN{H<:AbstractHorde, F<:Function}
    
    # Weights
    Î¸::Matrix{Float32}
    # Secondary Weights
    h::Matrix{Float32}

    # updates
    Î”Î¸::Matrix{Float32}
    Î”h::Matrix{Float32}
    
    # Psi
    Î¨::Matrix{Float32}
    # Phi
    Î¦::Vector{Matrix{Float32}}
    # PhiPrime
    Î¦â€²::Vector{Matrix{Float32}}
    # H
    H::Vector{Matrix{Float32}}
    # deltaH
    Î”H::Vector{Matrix{Float32}}

    # Questions Graph (ð’³)
    # GVFs
    horde::H

    # Activation
    Ïƒ::F
end

GradientGVFN(in, horde, Ïƒ; initÎ¸=Flux.glorot_uniform) =
    GradientGVFN(
        initÎ¸(length(horde), in + length(horde)), # Î¸
        Flux.zeros(length(horde), in + length(horde)), # h
        Flux.zeros(length(horde), in + length(horde)), # Î”Î¸
        Flux.zeros(length(horde), in + length(horde)), # Î”h
        Flux.zeros(length(horde), in + length(horde)), # Î¨
        [Flux.zeros(length(horde), in + length(horde))], # Î¦ FIXME: These dimensions are definitely wrong
        [Flux.zeros(length(horde), in + length(horde))], # Î¦â€²
        [Flux.zeros((in + length(horde))*length(horde), (in + length(horde))*length(horde)) for i in 1:length(horde)], # H FIXME: These dimensions are probably wrong and need fixed to fit with the rest of the new notation.
        [Flux.zeros((in + length(horde))*length(horde), (in + length(horde))*length(horde)) for i in 1:length(horde)], # Î”H
        horde,
        Ïƒ
    )

(gvfn::GradientGVFN)(s_tp1, pred_t, type::Type{Prediction}) = gvfn.Ïƒ.(gvfn.Î¸*[s_tp1; pred_t], type)
(gvfn::GradientGVFN)(s_tp1, pred_t, type::Type{Derivative}) = gvfn.Ïƒ.(gvfn.Î¸*[s_tp1; pred_t], type)
(gvfn::GradientGVFN)(s_tp1, pred_t, type::Type{DoubleDerivative}) = gvfn.Ïƒ.(gvfn.Î¸*[s_tp1; pred_t], type)

# recur_roll(gvfn::GradientGVFN, state::Array{<:AbstractFloat, 1}, pred_init, type) = [gvfn(state, pred_init, type)]
# function recur_roll(gvfn::GradientGVFN, states, pred_init, type)
#     if length(states) == 1
#         return [gvfn(states[end], pred_init, type)]
#     end
#     preds = recur_roll(gvfn, states[1:(end-1)], pred_init, type)
#     push!(preds, gvfn(states[end], preds[end], type))
# end

function roll(gvfn::GradientGVFN, states, pred_init, type)
    preds = [gvfn(states[1], pred_init, type)]
    for i in 2:length(states)
        push!(preds, gvfn(states[i], preds[i-1], type))
    end
    return preds
end


is_cumulant(cumulant, j) = false
is_cumulant(cumulant::PredictionCumulant, j) = cumulant.idx == j

is_cumulant(gvfn::T, i, j) where {T <: AbstractGVFLayer} = is_cumulant(gvfn.horde[i].cumulant)

mutable struct RGTD <: LearningUpdate
    Î±::Float64
    Î²::Float64
end


function update!(gvfn::GradientGVFN{H}, lu::RGTD, h_init, states, env_state_tp1) where {H <: AbstractHorde}

    preds = roll(gvfn, states, h_init, Prediction)
    predsâ€² = roll(gvfn, states, h_init, Derivative)
    predsâ€²â€² = roll(gvfn, states, h_init, DoubleDerivative)

    
    
    
end

# function update!(gvfn::Flux.Recur{T}, lu::RTDC, h_init, states, env_state_tp1) where {T <: AbstractGVFLayer}

#     # Î± = lu.Î±

#     reset!(gvfn, h_init)
#     preds = gvfn.(states)
#     preds_t = preds[end-1]
#     preds_tilde = Flux.data(preds[end])

#     Ï• = jacobian(preds[end-1], params(gvfn.cell))
    

#     cumulants, discounts, Ï = get(gvfn.cell, preds_tilde, env_state_tp1)
#     targets = cumulants .+ discounts.*preds_tilde
#     Î´ = targets .- preds_t

#     prms = params(gvfn.cell)
#     # println(prms)
#     grads = Tracker.gradient(() ->mean(Î´.^2), prms)
#     for weights in prms
#         Flux.Tracker.update!(weights, -Î±.*grads[weights])
#     end



    
# end
