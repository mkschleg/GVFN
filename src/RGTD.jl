

struct Prediction end
struct Derivative end
struct DoubleDerivative end



sigmoid(x, type::Type{Prediction}) = sigmoid(x)
sigmoid(x, type::Type{Derivative}) = sigmoidâ€²(x)
sigmoid(x, type::Type{DoubleDerivative}) = sigmoidâ€²â€²(x)

Ïƒ(x, type::Type{Prediction}) = sigmoid(x)
Ïƒ(x, type::Type{Derivative}) = sigmoidâ€²(x)
Ïƒ(x, type::Type{DoubleDerivative}) = sigmoidâ€²â€²(x)

relu(x, type::Type{Prediction}) = Flux.relu(x)
relu(x, type::Type{Derivative}) = x > 0 ? one(x) : zero(x)
relu(x, type::Type{DoubleDerivative}) = 0

linear(x, type::Type{Prediction}) = identity(x)
linear(x, type::Type{Derivative}) = one(x) 
linear(x, type::Type{DoubleDerivative}) = 0



"""
    GradientGVFN

A GVFN only for implementing recurrent GTD, which can be used with multiple activation functions (sigmoid, relu, linear). We will use the following as short hand in all the following notes:
    - n âˆˆ Z : The number of GVFs in the GVFN
    - k âˆˆ Z : The number of inputs in the observations

Parameters:
    Î¸::Matrix{Float32} -> The weights of the function approximator to make the next hidden state (size: nÃ—(n+k)
    h::Matrix{Float32} -> The secondary weights for the GTD (TDC) update.
    
    Î”Î¸::Matrix{Float32} -> The change in weights (or the update matrix)
    Î”h::Matrix{Float32} -> The change in secondary weights (or the secondary update matrix)
    
    # Psi
    Î¨::Matrix{Float32} -> For storing the Hessian operations
    # Phi
    Î¦::Matrix{Float32} -> For storing the gradients calculated through time (n Ã— (n*(n+k)))
    # PhiPrime
    Î¦â€²::Matrix{Float32} -> For storing the gradients calculated through time for the s_{t+1}
    # H
    H::Vector{Matrix{Float32}} -> The Hessian Matrix (This will store the hessian vector product)
    # deltaH
    Î”H::Vector{Matrix{Float32}}
"""
mutable struct GradientGVFN{H<:AbstractHorde, F<:Function}
    
    # Weights
    Î¸::Matrix{Float32}
    # Secondary Weights
    h::Matrix{Float32}

    # Psi
    Î¨::Vector{Float32}

    # eta A helper variable for calculating efficiently n \by n(n+k)
    Î·::Matrix{Float32}
    Î¾::Vector{Float32}
    # Phi
    # Number of GVFs \times Number of weights (n*(n+length(s)))
    Î¦::Matrix{Float32}
    # PhiPrime
    Î¦â€²::Matrix{Float32}
    # H
    Hvp::Matrix{Float32}
    # deltaH

    # Questions Graph (ğ’³)
    # GVFs
    horde::H

    # Activation
    Ïƒ::F
    # number of observations:
    k::Int64
    # number of GVFs
    n::Int64
end

function GradientGVFN(in, horde, Ïƒ; initÎ¸=Flux.glorot_uniform)
    n = length(horde)
    npk = in + length(horde)
    GradientGVFN(
        initÎ¸(n, npk), # Î¸
        zeros(Float32, n, npk), # h
        zeros(Float32, n*npk), # Î¨
        zeros(Float32, n, n*npk), # Î·
        zeros(Float32, n), # Î¾
        zeros(Float32, n, n*npk), # Î¦ 
        zeros(Float32, n, n*npk), # Î¦â€²
        zeros(Float32, n, n*npk), # H
        horde,
        Ïƒ,
        in,
        n)
end

(gvfn::GradientGVFN)(s_tp1, pred_t, type::Type{Prediction}) = gvfn.Ïƒ.(gvfn.Î¸*[s_tp1; pred_t], type)
(gvfn::GradientGVFN)(s_tp1, pred_t, type::Type{Derivative}) = gvfn.Ïƒ.(gvfn.Î¸*[s_tp1; pred_t], type)
(gvfn::GradientGVFN)(s_tp1, pred_t, type::Type{DoubleDerivative}) = gvfn.Ïƒ.(gvfn.Î¸*[s_tp1; pred_t], type)

"""
    roll

This takes a gvfn, states, initial state, and a type and produces a list of preds with a function
"""
function roll(gvfn::GradientGVFN, states, pred_init, type)
    preds = [gvfn(states[1], pred_init, type)]
    for i in 2:length(states)
        push!(preds, gvfn(states[i], preds[i-1], type))
    end
    return preds
end


is_cumulant(cumulant, j) = false
is_cumulant(cumulant::PredictionCumulant, j) =
    cumulant.idx == j

is_cumulant(gvfn::T, i, j) where {T <: GradientGVFN} =
    is_cumulant(gvfn.horde[i].cumulant, j)

function is_cumulant_mat(gvfn::T) where {T<:GradientGVFN}
    C = BitMatrix(undef, gvfn.n, gvfn.n)
    for (i, j) in Iterators.product(1:gvfn.n, 1:gvfn.n)
        C[i,j] = is_cumulant(gvfn, i, j)
    end
    C
end

mutable struct RGTD <: LearningUpdate
    Î±::Float64
    Î²::Float64
end

"""
    _sum_kron_delta(lhs::matrix, rhs::vector)

    lhs = i \by npk*n, rhs = npk

lhs^i_{k,j} + rhs_j *Î´^Îº_{i,k}
"""
function _sum_kron_delta!(lhs, rhs, k, n)
    for i in 1:size(lhs)[1]
        idx = (n+k)*(i-1)+1
        lhs[i, idx:(idx+(n+k-1))] .+= rhs
    end
    return lhs
end

function _sum_kron_delta(lhs, rhs, k, n)
    ret = copy(lhs)
    for i in 1:size(lhs)[1]
        idx = (n+k)*(i-1)+1
        ret[i, idx:(idx+(n+k-1))] .+= rhs
    end
    ret
end

function update!(gvfn::GradientGVFN{H},
                 opt,
                 lu::RGTD,
                 h_init,
                 states,
                 env_state_tp1,
                 action_t=nothing,
                 b_prob::F=1.0f0) where {H <: AbstractHorde, F<:AbstractFloat}

    Î· = gvfn.Î·
    Ï• = gvfn.Î¦
    Ï•â€² = gvfn.Î¦â€²
    Î¨ = gvfn.Î¨
    Î¸ = gvfn.Î¸
    w = gvfn.h
    k = gvfn.k
    n = gvfn.n
    Î¾ = gvfn.Î¾
    hvp = gvfn.Hvp
    
    preds = roll(gvfn, states, h_init, Prediction)
    predsâ€² = roll(gvfn, states, h_init, Derivative)
    predsâ€²â€² = roll(gvfn, states, h_init, DoubleDerivative)

    preds_t = preds[end-1]
    preds_tilde = preds[end]

    # calculate the gradients:
    # Assume the gradients are zero at the beginning of input (I.e. truncated).
    fill!(Ï•, zero(eltype(Ï•)))
    fill!(Ï•', zero(eltype(Ï•)))
    fill!(Î¾, zero(eltype(Î¾)))
    fill!(hvp, zero(eltype(hvp)))


    xtp1 = [states[1]; h_init] # x_tp1 = [o_tp1, h_{t}]
    # xtp1 = [states[2]; preds[1]] # x_tp1 = [o_tp1, h_t]
    # Time Step t
    # Î· n\byn*(n+k)
    Î· .= _sum_kron_delta(Î¸[:, (k+1):end]*Ï•, xtp1, k, n) # tp1
    Ï•â€² .= predsâ€²[1] .* Î· # tp1
    # @show predsâ€²[1]
    # @show Î·

    # ut_{k,j} = Ï•(t)_{k,j}
    wx = w*xtp1
    # Î¾ is n+k
    term_1 = (predsâ€²â€²[1].*(Î¸[:, (k+1):end]*Î¾ .+ wx)) .* Î·
    term_2 = predsâ€²[1] .* _sum_kron_delta(Î¸[:, (k+1):end]*hvp .+ w[:, (k+1):end]*Ï•, [zero(states[1]); Î¾], n, k)

    hvp .= term_1 .+ term_2

    Î¾ .= predsâ€²[1].*(Î¸[:, (k+1):end]*Î¾ + wx)
    Ï• .= Ï•â€² # tp1 -> t
    for tp1 in 2:(length(states)-1)
        xtp1 = [states[tp1]; preds[tp1-1]] # x_tp1 = [o_t, h_{t-1}]
        Î· .= _sum_kron_delta(Î¸[:, (k+1):end]*Ï•, xtp1, k, n)
        Ï•â€² .= predsâ€²[tp1] .* Î·

        wx = w*xtp1
        # Î¾ is n+k
        term_1 = (predsâ€²â€²[tp1].*(Î¸[:, (k+1):end]*Î¾ .+ wx)) .* Î·
        term_2 = predsâ€²[tp1] .* _sum_kron_delta(Î¸[:, (k+1):end]*hvp .+ w[:, (k+1):end]*Ï•, [zero(states[1]); Î¾], n, k)

        
        hvp .= term_1 .+ term_2

        Î¾ .= predsâ€²[tp1].*(Î¸[:, (k+1):end]*Î¾ + wx)
        Ï• .= Ï•â€² # tp1 -> t
    end

    xtp1 = [states[end]; preds[end-1]] # x_tp1 = [o_t, h_{t-1}]
    Î· .= _sum_kron_delta(Î¸[:, (k+1):end]*Ï•, xtp1, k, n)
    Ï•â€² .= predsâ€²[end] .* Î·


    # println(states)
    # @show preds
    # @show Î¸
    # @show Ï•
    # @show Î·
    # @show predsâ€²[end]
    # @show Ï•â€²

    # println(Î¸)
    # println(Ï•)
    # println(Ï•â€²)

    # println(Ï•â€²)
    # println(Ï•)

    cumulants, discounts, Ï€_prob = get(gvfn.horde, action_t, env_state_tp1, preds_tilde)
    Ï = Ï€_prob ./ b_prob
    targets = cumulants .+ discounts.*preds_tilde
    Î´ =  targets .- preds_t
    # println(Î´)

    Ï•w = Ï•*reshape(w', length(w), 1)

    rs_Î¸ = reshape(Î¸', length(Î¸),)

    Î¨ .= sum((Ï.*Î´ .- Ï•w).*hvp; dims=1)[1,:]

    Î± = lu.Î±
    Î² = lu.Î²
    C = is_cumulant_mat(gvfn)
    
    rs_Î¸ .+= Î±.*(sum(((Ï.*Î´).*Ï• .- (Ï.*Ï•w) .* (C*Ï•â€² .+ discounts.*Ï•â€²)); dims=1)[1,:] .- Î¨)

    rs_w = reshape(w', length(w),)
    rs_w .+= Î².*sum(((Ï.*Î´ - Ï•w) .* Ï•); dims=1)[1,:]
    # println(w)
    # println(hvp)

end


function update!(model::SingleLayer, horde::AbstractHorde, opt, lu::RGTD, state_seq, env_state_tp1, action_t=nothing, b_prob=1.0; prms=nothing)

    v = model.(state_seq[end-1:end])
    v_prime_t = deriv(model, state_seq[end-1])

    c, Î³, Ï€_prob = get(horde, action_t, env_state_tp1, Flux.data(v[end]))
    Ï = Ï€_prob./b_prob
    Î´ = Ï.*tderror(v[end-1], c, Î³, Flux.data(v[end]))
    Î” = Î´.*v_prime_t
    model.W .-= apply!(opt, model.W, Î”*state_seq[end-1]')
    model.b .-= apply!(opt, model.b, Î”)
end

